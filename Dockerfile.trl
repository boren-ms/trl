

# ARG BASE_IMAGE=sramdevregistry.azurecr.io/jing-dev:a100_latest
ARG BASE_IMAGE=ptebic.azurecr.io/public/aifx/acpt/stable-ubuntu2004-cu118-py310-torch230:latest
FROM ${BASE_IMAGE}
LABEL description="Docker image for distilabel"


RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN uv pip install --upgrade pip
RUN uv pip install bs4 \
    nvidia-ml-py \
    accelerate \
    datasets \
    transformers==4.46.2 \
    librosa \
    soundfile \
    jiwer \
    wandb \
    backoff \
    fire \
    peft

RUN MAX_JOBS=20 pip install flash-attn --no-build-isolation
# ENV FLASH_ATTENTION_VERSION="2.5.2"
# RUN git clone https://github.com/HazyResearch/flash-attention \
#     && cd flash-attention && git checkout v$FLASH_ATTENTION_VERSION \
#     && python setup.py install \
#     && FLASH_ATTENTION_FORCE_BUILD=TRUE pip install . \
#     && cd csrc/fused_softmax && pip install . && cd ../../ \
#     && cd csrc/rotary && pip install . && cd ../../ \
#     && cd csrc/xentropy && pip install . && cd ../../ \
#     && cd csrc/layer_norm && pip install . && cd ../../ \
#     && cd csrc/fused_dense_lib && pip install . && cd ../../ \
#     && cd .. && rm -rf flash-attention
ARG WANDB_API_KEY
ENV WANDB_API_KEY=${WANDB_API_KEY}