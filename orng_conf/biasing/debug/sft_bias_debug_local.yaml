# report_to: [wandb]
learning_rate: 1.0e-5
lr_scheduler_type: cosine
output_dir: ~/outputs/debug
# num_train_epochs: 1
max_steps: 100
per_device_train_batch_size: 2
gradient_accumulation_steps: 1
per_device_eval_batch_size: 16
gradient_checkpointing: True
logging_steps: 1
new_run: True
eos_token: "<|end|>"
bf16: True
max_length: 6512
# model_name_or_path: az://orngcresco/data/boren/data/ckp/phi4_mm_bias
# model_name_or_path: /home/boren/data/ckp/hf_models/phi4_mm_bias
model_name_or_path: /home/boren/data/ckp/hf_models/Phi-4-multimodal-instruct
save_steps: 50
# beta: 0.0
accelerator_config:
    dispatch_batches: False # disable dispatch for iterable datasets, need to handle sharding by ourself.
train_data:
    dataset_name: tsv
    # nug_egs: 5000
    # tsv_paths: az://orngscuscresco/data/boren/data/LibriSpeech/debug.tsv
    # tsv_paths: az://orngscuscresco/data/boren/data/LibriSpeech/ls_30k_shuf.tsv
    # tsv_paths: az://orngscuscresco/data/boren/data/LibriSpeech/debug.tsv
    tsv_paths: /home/boren/data/LibriSpeech/debug.tsv
    # dataset_name: jsonl
    # jsonl_paths: /home/boren/data/ckp/hf_models/Phi-4-multimodal-instruct/metric_vllm_ls_train_debug_biasing_smp2__results.jsonl
    # wer_filter:
    #     wer_range: [0.1, 100]
    # bwer_range: [1, 100]
    # format_preference:
    #     chosen_key: ref
    #     rejected_key: hyp
    post_process:
        # streaming: True
        # do_shard: True
        num_egs: 100
        streaming: True
        do_shard: True
        # path_map:
        #     src_part: /root/data/
        #     dst_part: /home/boren/data/
eval_strategy: steps
eval_on_start: True
eval_steps: 50
eval_data:
    - nickname: clean_no
      num_egs: 10
      dataset_name: ls_bias
      jsonl_path: /home/boren/data/librispeech_biasing/ref/test-clean.biasing_100.jsonl
    #   format_preference:
    #       chosen_key: text
    #       rejected_key: text
