# report_to: [wandb]
learning_rate: 1.0e-5
lr_scheduler_type: cosine
output_dir: ~/outputs/debug
# num_train_epochs: 1
max_steps: 1000
per_device_train_batch_size: 16
gradient_accumulation_steps: 4
per_device_eval_batch_size: 16
gradient_checkpointing: True
logging_steps: 1
use_logits_to_keep: True
# reference_free: True
use_liger_loss: True
bf16: True
# loss_type: ["sigmoid", "bco_pair", "sft"]
# loss_weights: [0.8, 0.2, 1.0]
max_prompt_length: 6000
max_completion_length: 512
# wandb_log_unique_prompts: True
# model_name_or_path: az://orngcresco/data/boren/data/ckp/phi4_mm_bias
# model_name_or_path: /home/boren/data/ckp/hf_models/phi4_mm_bias
model_name_or_path: /home/boren/data/ckp/hf_models/Phi-4-multimodal-instruct
new_lora: True
save_steps: 50
# beta: 0.0
# accelerator_config:
#     dispatch_batches: False # disable dispatch for iterable datasets, need to handle sharding by ourself.
train_data:
    dataset_name: jsonl
    # num_egs: 5000
    jsonl_paths: /home/boren/data/ckp/hf_models/Phi-4-multimodal-instruct/metric_vllm_ls_train_debug_biasing_smp2__results.jsonl
    wer_filter:
        wer_range: [0.1, 100]
        # bwer_range: [1, 100]
    format_preference:
        chosen_key: ref
        rejected_key: hyp
    post_process:
        # streaming: True
        # do_shard: True
        num_egs: 100
        streaming: True
        do_shard: True
        # path_map:
        #     src_part: /root/data/
        #     dst_part: /home/boren/data/
eval_strategy: steps
eval_on_start: True
eval_steps: 10
eval_data:
    - nickname: clean_no
      num_egs: 10
      dataset_name: ls_bias
      jsonl_path: /home/boren/data/librispeech_biasing/ref/test-clean.biasing_100.jsonl
      format_preference:
          chosen_key: text
          rejected_key: text
